{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unsupervised_LSTM_president_simulator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPlHbIv1JI+oJPyw/xw+J0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PseudoPythonista/nlp/blob/master/unsupervised_LSTM_president_simulator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiXlHw4ClbZa",
        "colab_type": "text"
      },
      "source": [
        "**Are you a president in need of a nonsensical speech?**\n",
        "\n",
        "* inspired by: https://www.youtube.com/watch?v=EFHyzuqjaok\n",
        "* data scraped used BeautifulSoup from: https://www.rev.com/blog transcript-category/donald-trump-transcripts\n",
        "* code based on a udemy course: https://www.udemy.com/course/pytorch-for-deep-learning-with-python-bootcamp/\n",
        "* works best with gpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-MDsubqu4Zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLeDKmkivM5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/clean_super_ultimate_speech.txt','r',encoding='utf8') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEkfkV0ZvY8R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "f13b7225-3d3b-456c-81de-ba4efe51c678"
      },
      "source": [
        "text[:500]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'President :  … number that you can have, and that’s all we’re allowed. So you’re a very special group, and thank you all for coming out here. Thank you, thank you. So I’m thrilled to be back in Ohio, and proud of the hardworking patriots of the Buckeye State. Four years ago, I came here, right in here. That was an amazing four years ago, wasn’t it, when you think how time flies? Right here in Cleveland, to accept the Republican nomination for President of the United States. And we’ve worked hard'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TvzYQ-OvdZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "87e99176-3c88-40f1-ff7c-8d4282e81e62"
      },
      "source": [
        "len(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2277900"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wshimmAW4Vmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_characters = set(text)\n",
        "decoder = dict(enumerate(all_characters))\n",
        "encoder = {char: ind for ind,char in decoder.items()}\n",
        "encoded_text = np.array([encoder[char] for char in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-uZKHGivfH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoder(encoded_text, num_uni_chars):\n",
        "    #encoded_text -> batch of encoded text\n",
        "    #num_uni_chars -> number of unique characters (len(set(text)))\n",
        "\n",
        "    one_hot = np.zeros((encoded_text.size, num_uni_chars)) # creates a matrix of zeros\n",
        "    one_hot = one_hot.astype(np.float32) #convert to float32 to avoid errors\n",
        "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0 #fancy idexing - fills the matrix at correct positions\n",
        "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars)) # reshaped to match the batch shape\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoSgrG4gxbt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
        "    \n",
        "\n",
        "    # x: Encoded Text of length seq_len\n",
        "    # y: Encoded Text shifted by one\n",
        "    \n",
        "    # encoded_text : Complete Encoded Text to make batches from\n",
        "    # batch_size : Number of samples per batch\n",
        "    # seq_len : Length of character sequence\n",
        "\n",
        "\n",
        "    char_per_batch = samp_per_batch * seq_len #n of chars per batch\n",
        "    num_batches_avail = int(len(encoded_text)/char_per_batch)#n of full batches available\n",
        "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]#drops the end that doesnt fit\n",
        "    encoded_text = encoded_text.reshape((samp_per_batch, -1))#reshape the txt into rows the size of a batch\n",
        "    \n",
        "    for n in range(0, encoded_text.shape[1], seq_len): #goes thru each row\n",
        "        \n",
        "        x = encoded_text[:, n:n+seq_len] # grabs a sequence of chars\n",
        "        y = np.zeros_like(x) #zero array shaped like x\n",
        "       \n",
        "        try:#y sequence like x but shifted by 1\n",
        "            y[:, :-1] = x[:, 1:] \n",
        "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
        "                \n",
        "        except:#potential indexing error at the end\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:, -1] = encoded_text[:, 0]\n",
        "            \n",
        "        yield x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9yRy6UC4GKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ad15240c-a2e3-48ae-9bc5-9e7d55b0dff3"
      },
      "source": [
        "sample = encoded_text[:20]\n",
        "sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([24, 82, 25, 38,  6, 62, 25, 55, 68, 17, 39, 17, 17,  9, 17, 55, 80,\n",
              "       54,  7, 25])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSTpBYce4g5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_generator = generate_batches(sample,samp_per_batch=2,seq_len=5)\n",
        "x,y = next(batch_generator)#one step thru the generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fV7P5QS46Sb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f49bd417-3780-4b3d-f6da-96f56377ce6a"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[24, 82, 25, 38,  6],\n",
              "       [39, 17, 17,  9, 17]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH-qsCYy486t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1eea28bd-2266-4a58-c77a-da61eb74d9c9"
      },
      "source": [
        "y# the same sequence as x but shifted by 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[82, 25, 38,  6, 62],\n",
              "       [17, 17,  9, 17, 55]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skBHvp_2xv8J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2fddbfa7-e164-44b7-b978-f39a36e2f339"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlmIj-Dc5W4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
        "        \n",
        "        \n",
        "        # SET UP ATTRIBUTES\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.use_gpu = use_gpu\n",
        "        \n",
        "        self.all_chars = all_chars\n",
        "        self.decoder = dict(enumerate(all_chars)) #internal decoder\n",
        "        self.encoder = {char: ind for ind,char in decoder.items()} #internal encoder\n",
        "                                                                                            #if True input and output provided at batch,seq,feature (the same format as the batch generator)\n",
        "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob) #dropout layer\n",
        "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "                  \n",
        "        lstm_output, hidden = self.lstm(x, hidden)\n",
        "        drop_output = self.dropout(lstm_output)\n",
        "        drop_output = drop_output.contiguous().view(-1, self.num_hidden) #needs to be reshaped to be connected to the last layer\n",
        "        final_out = self.fc_linear(drop_output)\n",
        "\n",
        "        return final_out, hidden\n",
        "    \n",
        "    def hidden_state(self, batch_size): #separate method to take gpu/cpu into account \n",
        "       \n",
        "        if self.use_gpu:    \n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
        "        else:\n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siapT1zaARql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CharModel(\n",
        "    all_chars=all_characters,\n",
        "    num_hidden=332,\n",
        "    num_layers=3,\n",
        "    drop_prob=0.5,\n",
        "    use_gpu=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMctFttYAXMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_param  = [] #ideally the num of params the same as len(text)\n",
        "for p in model.parameters():\n",
        "    total_param.append(int(p.numel()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OYtx442AjV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5a828253-fbdb-4154-b439-bd1fe6d5dcd7"
      },
      "source": [
        "sum(total_param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2365260"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCdd7KNKAm8d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "04195806-9316-48f7-cb52-8aec9515c572"
      },
      "source": [
        "len(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2277900"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r6HoLWbAuzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYJlDhQwAzrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_percent = 0.9\n",
        "train_ind = int(len(encoded_text) * (train_percent)) #to cut off the data for training\n",
        "\n",
        "train_data = encoded_text[:train_ind]\n",
        "val_data = encoded_text[train_ind:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otj-Hu-eBL1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 50\n",
        "batch_size = 128\n",
        "seq_len = 100 # Length of sequence\n",
        "\n",
        "#printing starts at 0\n",
        "tracker = 0\n",
        "\n",
        "# number of characters in text\n",
        "num_char = max(encoded_text)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00PxXfRICdvB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60eae989-178c-4fbf-aa9d-9f2749a4db4b"
      },
      "source": [
        "#training section\n",
        "\n",
        "model.train()\n",
        "\n",
        "if model.use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "for i in range(epochs):\n",
        "    \n",
        "    hidden = model.hidden_state(batch_size)\n",
        "    \n",
        "    \n",
        "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
        "        \n",
        "        tracker += 1\n",
        "        \n",
        "        x = one_hot_encoder(x,num_char)#one-hot encoded data\n",
        "        \n",
        "        inputs = torch.from_numpy(x) # arrays converted to tensors\n",
        "        targets = torch.from_numpy(y)\n",
        "        \n",
        "        if model.use_gpu:\n",
        "            \n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda()\n",
        "            \n",
        "\n",
        "        hidden = tuple([state.data for state in hidden]) #resets the hidden state, so it doesn´t backpropagate\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        lstm_output, hidden = model.forward(inputs,hidden)\n",
        "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)#clips gradients so they don´t explode\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        if tracker % 25 == 0:\n",
        "            \n",
        "            val_hidden = model.hidden_state(batch_size)\n",
        "            val_losses = []\n",
        "\n",
        "            #training section\n",
        "\n",
        "            model.eval()\n",
        "            \n",
        "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
        "                \n",
        "                x = one_hot_encoder(x,num_char) #one-hot encoded data\n",
        "                \n",
        "                inputs = torch.from_numpy(x) # arrays converted to tensor\n",
        "                targets = torch.from_numpy(y)\n",
        "\n",
        "                if model.use_gpu:\n",
        "\n",
        "                    inputs = inputs.cuda()\n",
        "                    targets = targets.cuda()\n",
        "                    \n",
        "                val_hidden = tuple([state.data for state in val_hidden])#again resets hidden\n",
        "                \n",
        "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
        "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "        \n",
        "                val_losses.append(val_loss.item())\n",
        "            \n",
        "            model.train() #resets back into training mode\n",
        "            \n",
        "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Step: 25 Val Loss: 3.112819194793701\n",
            "Epoch: 0 Step: 50 Val Loss: 3.10469126701355\n",
            "Epoch: 0 Step: 75 Val Loss: 3.1051485538482666\n",
            "Epoch: 0 Step: 100 Val Loss: 3.1035537719726562\n",
            "Epoch: 0 Step: 125 Val Loss: 3.1016550064086914\n",
            "Epoch: 0 Step: 150 Val Loss: 3.103311777114868\n",
            "Epoch: 1 Step: 175 Val Loss: 3.0972135066986084\n",
            "Epoch: 1 Step: 200 Val Loss: 3.0428037643432617\n",
            "Epoch: 1 Step: 225 Val Loss: 2.831845760345459\n",
            "Epoch: 1 Step: 250 Val Loss: 2.6896631717681885\n",
            "Epoch: 1 Step: 275 Val Loss: 2.578984260559082\n",
            "Epoch: 1 Step: 300 Val Loss: 2.4870636463165283\n",
            "Epoch: 2 Step: 325 Val Loss: 2.4149580001831055\n",
            "Epoch: 2 Step: 350 Val Loss: 2.3701016902923584\n",
            "Epoch: 2 Step: 375 Val Loss: 2.3276145458221436\n",
            "Epoch: 2 Step: 400 Val Loss: 2.297665596008301\n",
            "Epoch: 2 Step: 425 Val Loss: 2.257132053375244\n",
            "Epoch: 2 Step: 450 Val Loss: 2.2191321849823\n",
            "Epoch: 2 Step: 475 Val Loss: 2.183788537979126\n",
            "Epoch: 3 Step: 500 Val Loss: 2.150916814804077\n",
            "Epoch: 3 Step: 525 Val Loss: 2.1153910160064697\n",
            "Epoch: 3 Step: 550 Val Loss: 2.08830189704895\n",
            "Epoch: 3 Step: 575 Val Loss: 2.0638182163238525\n",
            "Epoch: 3 Step: 600 Val Loss: 2.0354621410369873\n",
            "Epoch: 3 Step: 625 Val Loss: 2.0127222537994385\n",
            "Epoch: 4 Step: 650 Val Loss: 1.9892545938491821\n",
            "Epoch: 4 Step: 675 Val Loss: 1.973313331604004\n",
            "Epoch: 4 Step: 700 Val Loss: 1.9489206075668335\n",
            "Epoch: 4 Step: 725 Val Loss: 1.9275174140930176\n",
            "Epoch: 4 Step: 750 Val Loss: 1.9053263664245605\n",
            "Epoch: 4 Step: 775 Val Loss: 1.886480689048767\n",
            "Epoch: 4 Step: 800 Val Loss: 1.8674191236495972\n",
            "Epoch: 5 Step: 825 Val Loss: 1.8487392663955688\n",
            "Epoch: 5 Step: 850 Val Loss: 1.8299622535705566\n",
            "Epoch: 5 Step: 875 Val Loss: 1.8158694505691528\n",
            "Epoch: 5 Step: 900 Val Loss: 1.8020575046539307\n",
            "Epoch: 5 Step: 925 Val Loss: 1.778327226638794\n",
            "Epoch: 5 Step: 950 Val Loss: 1.7691311836242676\n",
            "Epoch: 6 Step: 975 Val Loss: 1.7499500513076782\n",
            "Epoch: 6 Step: 1000 Val Loss: 1.7360109090805054\n",
            "Epoch: 6 Step: 1025 Val Loss: 1.7248455286026\n",
            "Epoch: 6 Step: 1050 Val Loss: 1.7102307081222534\n",
            "Epoch: 6 Step: 1075 Val Loss: 1.6930240392684937\n",
            "Epoch: 6 Step: 1100 Val Loss: 1.6818287372589111\n",
            "Epoch: 7 Step: 1125 Val Loss: 1.6725258827209473\n",
            "Epoch: 7 Step: 1150 Val Loss: 1.6585383415222168\n",
            "Epoch: 7 Step: 1175 Val Loss: 1.6481263637542725\n",
            "Epoch: 7 Step: 1200 Val Loss: 1.6355164051055908\n",
            "Epoch: 7 Step: 1225 Val Loss: 1.625514030456543\n",
            "Epoch: 7 Step: 1250 Val Loss: 1.6120179891586304\n",
            "Epoch: 7 Step: 1275 Val Loss: 1.6017934083938599\n",
            "Epoch: 8 Step: 1300 Val Loss: 1.5951095819473267\n",
            "Epoch: 8 Step: 1325 Val Loss: 1.584690809249878\n",
            "Epoch: 8 Step: 1350 Val Loss: 1.5733038187026978\n",
            "Epoch: 8 Step: 1375 Val Loss: 1.562265157699585\n",
            "Epoch: 8 Step: 1400 Val Loss: 1.5541958808898926\n",
            "Epoch: 8 Step: 1425 Val Loss: 1.5442874431610107\n",
            "Epoch: 9 Step: 1450 Val Loss: 1.5340040922164917\n",
            "Epoch: 9 Step: 1475 Val Loss: 1.5298242568969727\n",
            "Epoch: 9 Step: 1500 Val Loss: 1.520051121711731\n",
            "Epoch: 9 Step: 1525 Val Loss: 1.5118260383605957\n",
            "Epoch: 9 Step: 1550 Val Loss: 1.5045075416564941\n",
            "Epoch: 9 Step: 1575 Val Loss: 1.494989037513733\n",
            "Epoch: 9 Step: 1600 Val Loss: 1.4875624179840088\n",
            "Epoch: 10 Step: 1625 Val Loss: 1.482329249382019\n",
            "Epoch: 10 Step: 1650 Val Loss: 1.4732683897018433\n",
            "Epoch: 10 Step: 1675 Val Loss: 1.4673880338668823\n",
            "Epoch: 10 Step: 1700 Val Loss: 1.4582405090332031\n",
            "Epoch: 10 Step: 1725 Val Loss: 1.451795220375061\n",
            "Epoch: 10 Step: 1750 Val Loss: 1.4487097263336182\n",
            "Epoch: 11 Step: 1775 Val Loss: 1.4421104192733765\n",
            "Epoch: 11 Step: 1800 Val Loss: 1.4341506958007812\n",
            "Epoch: 11 Step: 1825 Val Loss: 1.4291807413101196\n",
            "Epoch: 11 Step: 1850 Val Loss: 1.4223617315292358\n",
            "Epoch: 11 Step: 1875 Val Loss: 1.4150623083114624\n",
            "Epoch: 11 Step: 1900 Val Loss: 1.4092755317687988\n",
            "Epoch: 12 Step: 1925 Val Loss: 1.406661033630371\n",
            "Epoch: 12 Step: 1950 Val Loss: 1.3998740911483765\n",
            "Epoch: 12 Step: 1975 Val Loss: 1.3964338302612305\n",
            "Epoch: 12 Step: 2000 Val Loss: 1.3899568319320679\n",
            "Epoch: 12 Step: 2025 Val Loss: 1.38124680519104\n",
            "Epoch: 12 Step: 2050 Val Loss: 1.377915382385254\n",
            "Epoch: 12 Step: 2075 Val Loss: 1.3779793977737427\n",
            "Epoch: 13 Step: 2100 Val Loss: 1.372749924659729\n",
            "Epoch: 13 Step: 2125 Val Loss: 1.3668122291564941\n",
            "Epoch: 13 Step: 2150 Val Loss: 1.3625761270523071\n",
            "Epoch: 13 Step: 2175 Val Loss: 1.3565034866333008\n",
            "Epoch: 13 Step: 2200 Val Loss: 1.3552922010421753\n",
            "Epoch: 13 Step: 2225 Val Loss: 1.3517504930496216\n",
            "Epoch: 14 Step: 2250 Val Loss: 1.347988247871399\n",
            "Epoch: 14 Step: 2275 Val Loss: 1.3456038236618042\n",
            "Epoch: 14 Step: 2300 Val Loss: 1.3395625352859497\n",
            "Epoch: 14 Step: 2325 Val Loss: 1.3338006734848022\n",
            "Epoch: 14 Step: 2350 Val Loss: 1.3296611309051514\n",
            "Epoch: 14 Step: 2375 Val Loss: 1.332922339439392\n",
            "Epoch: 14 Step: 2400 Val Loss: 1.3264764547348022\n",
            "Epoch: 15 Step: 2425 Val Loss: 1.3257336616516113\n",
            "Epoch: 15 Step: 2450 Val Loss: 1.3209774494171143\n",
            "Epoch: 15 Step: 2475 Val Loss: 1.319709062576294\n",
            "Epoch: 15 Step: 2500 Val Loss: 1.3137736320495605\n",
            "Epoch: 15 Step: 2525 Val Loss: 1.3094977140426636\n",
            "Epoch: 15 Step: 2550 Val Loss: 1.3063479661941528\n",
            "Epoch: 16 Step: 2575 Val Loss: 1.3072423934936523\n",
            "Epoch: 16 Step: 2600 Val Loss: 1.3037827014923096\n",
            "Epoch: 16 Step: 2625 Val Loss: 1.3012244701385498\n",
            "Epoch: 16 Step: 2650 Val Loss: 1.2966254949569702\n",
            "Epoch: 16 Step: 2675 Val Loss: 1.292801856994629\n",
            "Epoch: 16 Step: 2700 Val Loss: 1.2935166358947754\n",
            "Epoch: 17 Step: 2725 Val Loss: 1.2894723415374756\n",
            "Epoch: 17 Step: 2750 Val Loss: 1.288606882095337\n",
            "Epoch: 17 Step: 2775 Val Loss: 1.2831292152404785\n",
            "Epoch: 17 Step: 2800 Val Loss: 1.2845762968063354\n",
            "Epoch: 17 Step: 2825 Val Loss: 1.279537558555603\n",
            "Epoch: 17 Step: 2850 Val Loss: 1.278018832206726\n",
            "Epoch: 17 Step: 2875 Val Loss: 1.2761719226837158\n",
            "Epoch: 18 Step: 2900 Val Loss: 1.2749407291412354\n",
            "Epoch: 18 Step: 2925 Val Loss: 1.272768497467041\n",
            "Epoch: 18 Step: 2950 Val Loss: 1.2687407732009888\n",
            "Epoch: 18 Step: 2975 Val Loss: 1.2671347856521606\n",
            "Epoch: 18 Step: 3000 Val Loss: 1.264760971069336\n",
            "Epoch: 18 Step: 3025 Val Loss: 1.2644453048706055\n",
            "Epoch: 19 Step: 3050 Val Loss: 1.2632575035095215\n",
            "Epoch: 19 Step: 3075 Val Loss: 1.2618038654327393\n",
            "Epoch: 19 Step: 3100 Val Loss: 1.260628581047058\n",
            "Epoch: 19 Step: 3125 Val Loss: 1.2584388256072998\n",
            "Epoch: 19 Step: 3150 Val Loss: 1.2547008991241455\n",
            "Epoch: 19 Step: 3175 Val Loss: 1.252066969871521\n",
            "Epoch: 19 Step: 3200 Val Loss: 1.2548140287399292\n",
            "Epoch: 20 Step: 3225 Val Loss: 1.2515630722045898\n",
            "Epoch: 20 Step: 3250 Val Loss: 1.2482203245162964\n",
            "Epoch: 20 Step: 3275 Val Loss: 1.2485640048980713\n",
            "Epoch: 20 Step: 3300 Val Loss: 1.2468287944793701\n",
            "Epoch: 20 Step: 3325 Val Loss: 1.2434017658233643\n",
            "Epoch: 20 Step: 3350 Val Loss: 1.2431212663650513\n",
            "Epoch: 21 Step: 3375 Val Loss: 1.2419569492340088\n",
            "Epoch: 21 Step: 3400 Val Loss: 1.2399966716766357\n",
            "Epoch: 21 Step: 3425 Val Loss: 1.2375043630599976\n",
            "Epoch: 21 Step: 3450 Val Loss: 1.2340315580368042\n",
            "Epoch: 21 Step: 3475 Val Loss: 1.2320873737335205\n",
            "Epoch: 21 Step: 3500 Val Loss: 1.2332409620285034\n",
            "Epoch: 22 Step: 3525 Val Loss: 1.231765866279602\n",
            "Epoch: 22 Step: 3550 Val Loss: 1.23284113407135\n",
            "Epoch: 22 Step: 3575 Val Loss: 1.2296220064163208\n",
            "Epoch: 22 Step: 3600 Val Loss: 1.2291125059127808\n",
            "Epoch: 22 Step: 3625 Val Loss: 1.2262513637542725\n",
            "Epoch: 22 Step: 3650 Val Loss: 1.2249674797058105\n",
            "Epoch: 22 Step: 3675 Val Loss: 1.2263282537460327\n",
            "Epoch: 23 Step: 3700 Val Loss: 1.2253270149230957\n",
            "Epoch: 23 Step: 3725 Val Loss: 1.2232826948165894\n",
            "Epoch: 23 Step: 3750 Val Loss: 1.2225325107574463\n",
            "Epoch: 23 Step: 3775 Val Loss: 1.2191497087478638\n",
            "Epoch: 23 Step: 3800 Val Loss: 1.21693754196167\n",
            "Epoch: 23 Step: 3825 Val Loss: 1.219337821006775\n",
            "Epoch: 24 Step: 3850 Val Loss: 1.2180781364440918\n",
            "Epoch: 24 Step: 3875 Val Loss: 1.2167984247207642\n",
            "Epoch: 24 Step: 3900 Val Loss: 1.2143193483352661\n",
            "Epoch: 24 Step: 3925 Val Loss: 1.214910864830017\n",
            "Epoch: 24 Step: 3950 Val Loss: 1.21112859249115\n",
            "Epoch: 24 Step: 3975 Val Loss: 1.211692452430725\n",
            "Epoch: 24 Step: 4000 Val Loss: 1.2121235132217407\n",
            "Epoch: 25 Step: 4025 Val Loss: 1.2141470909118652\n",
            "Epoch: 25 Step: 4050 Val Loss: 1.2116869688034058\n",
            "Epoch: 25 Step: 4075 Val Loss: 1.20866060256958\n",
            "Epoch: 25 Step: 4100 Val Loss: 1.2078924179077148\n",
            "Epoch: 25 Step: 4125 Val Loss: 1.208167314529419\n",
            "Epoch: 25 Step: 4150 Val Loss: 1.2061357498168945\n",
            "Epoch: 26 Step: 4175 Val Loss: 1.2081445455551147\n",
            "Epoch: 26 Step: 4200 Val Loss: 1.2054287195205688\n",
            "Epoch: 26 Step: 4225 Val Loss: 1.2033133506774902\n",
            "Epoch: 26 Step: 4250 Val Loss: 1.2012577056884766\n",
            "Epoch: 26 Step: 4275 Val Loss: 1.2000013589859009\n",
            "Epoch: 26 Step: 4300 Val Loss: 1.199734091758728\n",
            "Epoch: 27 Step: 4325 Val Loss: 1.2003692388534546\n",
            "Epoch: 27 Step: 4350 Val Loss: 1.2006696462631226\n",
            "Epoch: 27 Step: 4375 Val Loss: 1.1994572877883911\n",
            "Epoch: 27 Step: 4400 Val Loss: 1.200026512145996\n",
            "Epoch: 27 Step: 4425 Val Loss: 1.1982507705688477\n",
            "Epoch: 27 Step: 4450 Val Loss: 1.1948238611221313\n",
            "Epoch: 27 Step: 4475 Val Loss: 1.1958824396133423\n",
            "Epoch: 28 Step: 4500 Val Loss: 1.1953680515289307\n",
            "Epoch: 28 Step: 4525 Val Loss: 1.1973801851272583\n",
            "Epoch: 28 Step: 4550 Val Loss: 1.1938340663909912\n",
            "Epoch: 28 Step: 4575 Val Loss: 1.1946896314620972\n",
            "Epoch: 28 Step: 4600 Val Loss: 1.1901413202285767\n",
            "Epoch: 28 Step: 4625 Val Loss: 1.1938095092773438\n",
            "Epoch: 29 Step: 4650 Val Loss: 1.1907626390457153\n",
            "Epoch: 29 Step: 4675 Val Loss: 1.1925404071807861\n",
            "Epoch: 29 Step: 4700 Val Loss: 1.1895338296890259\n",
            "Epoch: 29 Step: 4725 Val Loss: 1.18833589553833\n",
            "Epoch: 29 Step: 4750 Val Loss: 1.185245394706726\n",
            "Epoch: 29 Step: 4775 Val Loss: 1.188354730606079\n",
            "Epoch: 29 Step: 4800 Val Loss: 1.1852706670761108\n",
            "Epoch: 30 Step: 4825 Val Loss: 1.1886756420135498\n",
            "Epoch: 30 Step: 4850 Val Loss: 1.186618685722351\n",
            "Epoch: 30 Step: 4875 Val Loss: 1.1842209100723267\n",
            "Epoch: 30 Step: 4900 Val Loss: 1.182379961013794\n",
            "Epoch: 30 Step: 4925 Val Loss: 1.1826562881469727\n",
            "Epoch: 30 Step: 4950 Val Loss: 1.1821341514587402\n",
            "Epoch: 31 Step: 4975 Val Loss: 1.182450294494629\n",
            "Epoch: 31 Step: 5000 Val Loss: 1.1814424991607666\n",
            "Epoch: 31 Step: 5025 Val Loss: 1.1806234121322632\n",
            "Epoch: 31 Step: 5050 Val Loss: 1.177702784538269\n",
            "Epoch: 31 Step: 5075 Val Loss: 1.1803656816482544\n",
            "Epoch: 31 Step: 5100 Val Loss: 1.1784875392913818\n",
            "Epoch: 32 Step: 5125 Val Loss: 1.1767107248306274\n",
            "Epoch: 32 Step: 5150 Val Loss: 1.178425669670105\n",
            "Epoch: 32 Step: 5175 Val Loss: 1.1770977973937988\n",
            "Epoch: 32 Step: 5200 Val Loss: 1.1770107746124268\n",
            "Epoch: 32 Step: 5225 Val Loss: 1.1773159503936768\n",
            "Epoch: 32 Step: 5250 Val Loss: 1.1744091510772705\n",
            "Epoch: 32 Step: 5275 Val Loss: 1.1769238710403442\n",
            "Epoch: 33 Step: 5300 Val Loss: 1.1750930547714233\n",
            "Epoch: 33 Step: 5325 Val Loss: 1.17721426486969\n",
            "Epoch: 33 Step: 5350 Val Loss: 1.1739977598190308\n",
            "Epoch: 33 Step: 5375 Val Loss: 1.1733781099319458\n",
            "Epoch: 33 Step: 5400 Val Loss: 1.1688858270645142\n",
            "Epoch: 33 Step: 5425 Val Loss: 1.1715147495269775\n",
            "Epoch: 34 Step: 5450 Val Loss: 1.1705414056777954\n",
            "Epoch: 34 Step: 5475 Val Loss: 1.1718822717666626\n",
            "Epoch: 34 Step: 5500 Val Loss: 1.1701951026916504\n",
            "Epoch: 34 Step: 5525 Val Loss: 1.1682720184326172\n",
            "Epoch: 34 Step: 5550 Val Loss: 1.1659122705459595\n",
            "Epoch: 34 Step: 5575 Val Loss: 1.1682407855987549\n",
            "Epoch: 34 Step: 5600 Val Loss: 1.1672120094299316\n",
            "Epoch: 35 Step: 5625 Val Loss: 1.1674325466156006\n",
            "Epoch: 35 Step: 5650 Val Loss: 1.1683789491653442\n",
            "Epoch: 35 Step: 5675 Val Loss: 1.1648715734481812\n",
            "Epoch: 35 Step: 5700 Val Loss: 1.1649084091186523\n",
            "Epoch: 35 Step: 5725 Val Loss: 1.1661601066589355\n",
            "Epoch: 35 Step: 5750 Val Loss: 1.1651982069015503\n",
            "Epoch: 36 Step: 5775 Val Loss: 1.1674814224243164\n",
            "Epoch: 36 Step: 5800 Val Loss: 1.1654971837997437\n",
            "Epoch: 36 Step: 5825 Val Loss: 1.1623603105545044\n",
            "Epoch: 36 Step: 5850 Val Loss: 1.163249135017395\n",
            "Epoch: 36 Step: 5875 Val Loss: 1.1608279943466187\n",
            "Epoch: 36 Step: 5900 Val Loss: 1.1610625982284546\n",
            "Epoch: 37 Step: 5925 Val Loss: 1.161415457725525\n",
            "Epoch: 37 Step: 5950 Val Loss: 1.16059410572052\n",
            "Epoch: 37 Step: 5975 Val Loss: 1.15968918800354\n",
            "Epoch: 37 Step: 6000 Val Loss: 1.1606440544128418\n",
            "Epoch: 37 Step: 6025 Val Loss: 1.1597752571105957\n",
            "Epoch: 37 Step: 6050 Val Loss: 1.1583493947982788\n",
            "Epoch: 37 Step: 6075 Val Loss: 1.1589443683624268\n",
            "Epoch: 38 Step: 6100 Val Loss: 1.157698631286621\n",
            "Epoch: 38 Step: 6125 Val Loss: 1.1587038040161133\n",
            "Epoch: 38 Step: 6150 Val Loss: 1.158155083656311\n",
            "Epoch: 38 Step: 6175 Val Loss: 1.1585907936096191\n",
            "Epoch: 38 Step: 6200 Val Loss: 1.1541352272033691\n",
            "Epoch: 38 Step: 6225 Val Loss: 1.1551729440689087\n",
            "Epoch: 39 Step: 6250 Val Loss: 1.1544089317321777\n",
            "Epoch: 39 Step: 6275 Val Loss: 1.1565109491348267\n",
            "Epoch: 39 Step: 6300 Val Loss: 1.1549874544143677\n",
            "Epoch: 39 Step: 6325 Val Loss: 1.15418541431427\n",
            "Epoch: 39 Step: 6350 Val Loss: 1.149992823600769\n",
            "Epoch: 39 Step: 6375 Val Loss: 1.1524142026901245\n",
            "Epoch: 39 Step: 6400 Val Loss: 1.1508426666259766\n",
            "Epoch: 40 Step: 6425 Val Loss: 1.152605414390564\n",
            "Epoch: 40 Step: 6450 Val Loss: 1.1534212827682495\n",
            "Epoch: 40 Step: 6475 Val Loss: 1.151644229888916\n",
            "Epoch: 40 Step: 6500 Val Loss: 1.151479959487915\n",
            "Epoch: 40 Step: 6525 Val Loss: 1.1511926651000977\n",
            "Epoch: 40 Step: 6550 Val Loss: 1.1528053283691406\n",
            "Epoch: 41 Step: 6575 Val Loss: 1.15191650390625\n",
            "Epoch: 41 Step: 6600 Val Loss: 1.151487946510315\n",
            "Epoch: 41 Step: 6625 Val Loss: 1.1499714851379395\n",
            "Epoch: 41 Step: 6650 Val Loss: 1.1478885412216187\n",
            "Epoch: 41 Step: 6675 Val Loss: 1.1464589834213257\n",
            "Epoch: 41 Step: 6700 Val Loss: 1.1477473974227905\n",
            "Epoch: 42 Step: 6725 Val Loss: 1.1466453075408936\n",
            "Epoch: 42 Step: 6750 Val Loss: 1.1489230394363403\n",
            "Epoch: 42 Step: 6775 Val Loss: 1.1487092971801758\n",
            "Epoch: 42 Step: 6800 Val Loss: 1.1493349075317383\n",
            "Epoch: 42 Step: 6825 Val Loss: 1.1476316452026367\n",
            "Epoch: 42 Step: 6850 Val Loss: 1.146278738975525\n",
            "Epoch: 42 Step: 6875 Val Loss: 1.1479216814041138\n",
            "Epoch: 43 Step: 6900 Val Loss: 1.1463735103607178\n",
            "Epoch: 43 Step: 6925 Val Loss: 1.1463017463684082\n",
            "Epoch: 43 Step: 6950 Val Loss: 1.1459473371505737\n",
            "Epoch: 43 Step: 6975 Val Loss: 1.1485016345977783\n",
            "Epoch: 43 Step: 7000 Val Loss: 1.1448365449905396\n",
            "Epoch: 43 Step: 7025 Val Loss: 1.146007776260376\n",
            "Epoch: 44 Step: 7050 Val Loss: 1.143723487854004\n",
            "Epoch: 44 Step: 7075 Val Loss: 1.1450130939483643\n",
            "Epoch: 44 Step: 7100 Val Loss: 1.1452292203903198\n",
            "Epoch: 44 Step: 7125 Val Loss: 1.1434931755065918\n",
            "Epoch: 44 Step: 7150 Val Loss: 1.1412017345428467\n",
            "Epoch: 44 Step: 7175 Val Loss: 1.1414601802825928\n",
            "Epoch: 44 Step: 7200 Val Loss: 1.1401728391647339\n",
            "Epoch: 45 Step: 7225 Val Loss: 1.1405328512191772\n",
            "Epoch: 45 Step: 7250 Val Loss: 1.1445499658584595\n",
            "Epoch: 45 Step: 7275 Val Loss: 1.1395158767700195\n",
            "Epoch: 45 Step: 7300 Val Loss: 1.1396312713623047\n",
            "Epoch: 45 Step: 7325 Val Loss: 1.140260934829712\n",
            "Epoch: 45 Step: 7350 Val Loss: 1.1399798393249512\n",
            "Epoch: 46 Step: 7375 Val Loss: 1.1413933038711548\n",
            "Epoch: 46 Step: 7400 Val Loss: 1.1398742198944092\n",
            "Epoch: 46 Step: 7425 Val Loss: 1.1389940977096558\n",
            "Epoch: 46 Step: 7450 Val Loss: 1.1379711627960205\n",
            "Epoch: 46 Step: 7475 Val Loss: 1.136955738067627\n",
            "Epoch: 46 Step: 7500 Val Loss: 1.1363259553909302\n",
            "Epoch: 47 Step: 7525 Val Loss: 1.1368428468704224\n",
            "Epoch: 47 Step: 7550 Val Loss: 1.1392091512680054\n",
            "Epoch: 47 Step: 7575 Val Loss: 1.1369091272354126\n",
            "Epoch: 47 Step: 7600 Val Loss: 1.134697437286377\n",
            "Epoch: 47 Step: 7625 Val Loss: 1.1348137855529785\n",
            "Epoch: 47 Step: 7650 Val Loss: 1.135278344154358\n",
            "Epoch: 47 Step: 7675 Val Loss: 1.1355232000350952\n",
            "Epoch: 48 Step: 7700 Val Loss: 1.1356042623519897\n",
            "Epoch: 48 Step: 7725 Val Loss: 1.135266900062561\n",
            "Epoch: 48 Step: 7750 Val Loss: 1.1335140466690063\n",
            "Epoch: 48 Step: 7775 Val Loss: 1.1338019371032715\n",
            "Epoch: 48 Step: 7800 Val Loss: 1.131461501121521\n",
            "Epoch: 48 Step: 7825 Val Loss: 1.1341118812561035\n",
            "Epoch: 49 Step: 7850 Val Loss: 1.1319078207015991\n",
            "Epoch: 49 Step: 7875 Val Loss: 1.1322615146636963\n",
            "Epoch: 49 Step: 7900 Val Loss: 1.131904125213623\n",
            "Epoch: 49 Step: 7925 Val Loss: 1.131568193435669\n",
            "Epoch: 49 Step: 7950 Val Loss: 1.128178358078003\n",
            "Epoch: 49 Step: 7975 Val Loss: 1.1310604810714722\n",
            "Epoch: 49 Step: 8000 Val Loss: 1.129569172859192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0g2yUxsUe4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_next_char(model, char, hidden=None, k=1):\n",
        "        \n",
        "        encoded_text = model.encoder[char] #will use chars in seed to predict following letters\n",
        "        encoded_text = np.array([[encoded_text]])#convert to array for one-hot encoding\n",
        "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
        "        \n",
        "        inputs = torch.from_numpy(encoded_text)#array to tensor\n",
        "\n",
        "        if(model.use_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        hidden = tuple([state.data for state in hidden]) #hidden states for the model\n",
        "        \n",
        "        lstm_out, hidden = model(inputs, hidden)#get the output\n",
        "\n",
        "        probs = F.softmax(lstm_out, dim=1).data #lstm to probalities\n",
        "        \n",
        "        if(model.use_gpu):\n",
        "            probs = probs.cpu()# move back to CPU to use with numpy\n",
        "              \n",
        "        probs, index_positions = probs.topk(k)#number of chars prediction based on\n",
        "        \n",
        "        index_positions = index_positions.numpy().squeeze()\n",
        "        \n",
        "        probs = probs.numpy().flatten() # flattened array of probabilities\n",
        "        \n",
        "        probs = probs/probs.sum() #probs per index\n",
        "      \n",
        "        char = np.random.choice(index_positions, p=probs)  # randomly choose a character based on probabilities\n",
        "       \n",
        "        return model.decoder[char], hidden # return the encoded value of the predicted char and the hidden state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r1o7kqIZPU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, size, seed='Welcome', k=1): #prediction based on 1 top char \n",
        "    \n",
        "    if(model.use_gpu):\n",
        "        model.cuda()\n",
        "    else:\n",
        "        model.cpu()\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    output_chars = [c for c in seed] #output based on the seed\n",
        "    \n",
        "    # intiate hidden state\n",
        "    hidden = model.hidden_state(1)\n",
        "    \n",
        "    for char in seed: #predicts next char for every cher in seed\n",
        "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
        "    \n",
        "    output_chars.append(char) #adds initial chars to ouput\n",
        "    \n",
        "    for i in range(size): #repeats the operation for size provided\n",
        "        \n",
        "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k) # predict based off last letter in output_chars\n",
        "        \n",
        "        output_chars.append(char)# add predicted character\n",
        "        \n",
        "    return ''.join(output_chars) # return string of predicted text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeoEl44TatmS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7b6b8e87-2aba-4d52-a42e-0384d42460e9"
      },
      "source": [
        "with open(\"speech.txt\", \"w\") as text_file:\n",
        "  text_file.write(generate_text(model,1000, seed='Ohio', k=3))\n",
        "with open('/content/speech.txt','r',encoding='utf8') as f:\n",
        "  text = f.read()\n",
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ohio and I won our stuff, this was a great start. Thank you. Thank you. I said, “Washington. We are going to be the program. They wouldn’t have taken a great president. I said, “What’s too building a lot of things are standing up to tell me that. That’s a big person. It was truck to send our care of our children with the party of their country. We are trying to be the strongest than anybody that they’re dealing that we’re doing and we have the best is a sacrifice that we have to say, “What a group and any president. I don’t know, they don’t have to be saying. It’s the way, they hope you’re going to be a big deal. That’s what the party of this power of the second children. What are you had to do it about a good, and thank you, Mr. President. I said, “I want to thank this state of the women.:  It’s a lot of could go together where I was a long time, because I didn’t have to be a lot of people who stand in our country.:  They want to say, “That made the state of America with the world and we h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}